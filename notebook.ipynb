{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c87ce03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "from langgraph.graph.message import AnyMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph.message import add_messages, MessagesState\n",
    "from typing import Annotated\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "343aabdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562f7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f89f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b710b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77495135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a2d9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_call(state):\n",
    "    return {\"messages\" : [llm.invoke(state['messages'])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "947056a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10eb4d430>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.set_entry_point('node1')\n",
    "graph.add_node('node1', model_call)\n",
    "graph.add_edge('node1', END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bde526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57fe7c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langgraph.graph.state.CompiledStateGraph"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "011ebb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage('can you explain me how you get build')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c409c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.invoke({'messages': messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f9820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = [HumanMessage('ok now explain me how can i do it')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53da8b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='can you explain me how you get build', additional_kwargs={}, response_metadata={}, id='90e22518-79c9-4a65-b87e-4aa52f72b49d'),\n",
       " AIMessage(content='Okay, let\\'s break down how I, as a large language model, am \"built.\" It\\'s a multi-faceted process involving data, architecture, training, and ongoing refinement.  Think of it like building a house: you need materials (data), a blueprint (architecture), construction workers (training algorithms), and ongoing maintenance (refinement).\\n\\n**1. Data: The Foundation**\\n\\n*   **Massive Text and Code Datasets:** I am trained on an enormous amount of text and code data. This data comes from a variety of sources, including:\\n    *   **The Internet:**  Web pages, articles, blog posts, forums, and other publicly available text.\\n    *   **Books:**  A vast collection of published books.\\n    *   **Code Repositories:**  Code from platforms like GitHub, covering many programming languages.\\n    *   **Other Sources:**  Scientific papers, news articles, and other specialized datasets.\\n*   **Data Cleaning and Preprocessing:** The raw data is often messy and needs to be cleaned and preprocessed. This involves:\\n    *   **Removing irrelevant content:**  Filtering out noise, advertisements, and low-quality text.\\n    *   **Tokenization:**  Breaking down the text into smaller units called \"tokens\" (usually words or parts of words).\\n    *   **Normalization:**  Converting text to a consistent format (e.g., lowercase, removing punctuation).\\n    *   **Data Augmentation:**  Sometimes, the data is augmented to increase its diversity and improve the model\\'s robustness.\\n\\n**2. Architecture: The Blueprint**\\n\\n*   **Transformer Networks:**  The core architecture used in most modern large language models, including me, is based on the \"Transformer\" neural network architecture.\\n*   **Key Components of the Transformer:**\\n    *   **Attention Mechanism:**  This is the heart of the Transformer. It allows the model to focus on the most relevant parts of the input when processing it.  Instead of treating all words equally, the attention mechanism assigns weights to different words based on their importance in the context.\\n    *   **Encoder:**  Processes the input sequence and creates a representation of it.\\n    *   **Decoder:**  Generates the output sequence based on the encoded representation.\\n    *   **Multi-Head Attention:**  The attention mechanism is applied multiple times in parallel (\"multiple heads\") to capture different aspects of the input.\\n    *   **Feedforward Neural Networks:**  After the attention layers, feedforward neural networks are used to further process the information.\\n    *   **Residual Connections and Layer Normalization:**  These techniques help to stabilize training and improve performance.\\n*   **Scaling Up:**  Large language models are characterized by their massive size, often with billions or even trillions of parameters (the weights and biases in the neural network).  The more parameters, the more complex patterns the model can learn.\\n\\n**3. Training: The Construction Process**\\n\\n*   **Self-Supervised Learning:**  I am trained using a technique called \"self-supervised learning.\" This means that the model learns from the data itself, without explicit labels.\\n*   **Training Objectives:**  Common training objectives include:\\n    *   **Language Modeling:**  Predicting the next word in a sequence.  The model is given a sequence of words and asked to predict the word that comes next. This forces the model to learn the statistical relationships between words and phrases.\\n    *   **Masked Language Modeling (MLM):**  Randomly masking some of the words in a sequence and asking the model to predict the masked words.  This helps the model to understand the context of words and their relationships to each other.\\n    *   **Next Sentence Prediction (NSP):**  Given two sentences, predicting whether the second sentence follows the first.  This helps the model to understand the relationships between sentences and paragraphs.\\n*   **Optimization:**  The model\\'s parameters are adjusted during training to minimize the loss function (the difference between the model\\'s predictions and the actual values).  This is done using optimization algorithms like stochastic gradient descent (SGD) or Adam.\\n*   **Distributed Training:**  Training large language models requires massive computational resources.  The training process is typically distributed across many GPUs or TPUs (Tensor Processing Units) to speed it up.\\n\\n**4. Refinement: Ongoing Maintenance and Improvement**\\n\\n*   **Fine-Tuning:**  After the initial pre-training, the model can be fine-tuned on specific tasks or datasets.  This involves training the model on a smaller, more focused dataset to improve its performance on a particular task.  For example, a language model could be fine-tuned for question answering, text summarization, or machine translation.\\n*   **Reinforcement Learning from Human Feedback (RLHF):**  This is a technique used to align the model\\'s behavior with human preferences.  Human evaluators provide feedback on the model\\'s responses, and this feedback is used to train a reward model.  The reward model is then used to train the language model using reinforcement learning.\\n*   **Continuous Learning:**  The model is continuously updated with new data and improved training techniques.  This helps to keep the model up-to-date and improve its performance over time.\\n*   **Safety and Bias Mitigation:**  Efforts are made to mitigate biases in the model and ensure that it is used responsibly.  This involves carefully curating the training data, developing techniques to detect and remove biases, and implementing safety mechanisms to prevent the model from generating harmful or offensive content.\\n\\n**In Summary:**\\n\\nI am \"built\" through a complex process of:\\n\\n1.  **Gathering and preparing massive amounts of text and code data.**\\n2.  **Designing a powerful neural network architecture (Transformer).**\\n3.  **Training the model on the data using self-supervised learning.**\\n4.  **Refining the model through fine-tuning, reinforcement learning, and continuous learning.**\\n\\nThis process requires significant computational resources, expertise in machine learning, and ongoing research and development.  The field is constantly evolving, and new techniques are being developed to improve the performance, safety, and efficiency of large language models.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--4b4e6cf3-80a9-4c75-913c-3804d66c95ea-0', usage_metadata={'input_tokens': 8, 'output_tokens': 1300, 'total_tokens': 1308, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='ok now explain me how can i do it', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state['messages'] + new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89487c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke({'messages': state['messages'] + new_message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eee5740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down how you can build your own language model. Keep in mind that building a state-of-the-art model like me requires significant resources (data, compute, expertise). However, you can definitely build smaller, simpler models to learn the fundamentals and experiment. Here's a roadmap:\n",
      "\n",
      "**1. Define Your Goal and Scope:**\n",
      "\n",
      "*   **What do you want your model to do?**  Be specific.  Examples:\n",
      "    *   **Text generation:** Generate creative text, stories, poems, code, etc.\n",
      "    *   **Text classification:** Classify text into categories (e.g., spam detection, sentiment analysis).\n",
      "    *   **Question answering:** Answer questions based on a given context.\n",
      "    *   **Translation:** Translate text from one language to another.\n",
      "    *   **Summarization:** Summarize long documents.\n",
      "*   **What is your target domain?**  Will it be general-purpose, or focused on a specific area (e.g., medical text, legal documents, financial news)?\n",
      "*   **What resources do you have available?**  Consider your computing power (CPU, GPU), data storage, and time commitment.\n",
      "*   **What is your level of expertise?**  Are you a beginner, intermediate, or advanced programmer with machine learning experience?\n",
      "\n",
      "**2. Data Acquisition and Preparation:**\n",
      "\n",
      "*   **Find or Create a Dataset:**  The quality and quantity of your data are crucial.\n",
      "    *   **Public Datasets:** Start with publicly available datasets. Some popular options:\n",
      "        *   **Hugging Face Datasets:** A vast collection of datasets for various NLP tasks.\n",
      "        *   **Common Crawl:** A massive archive of web pages (requires significant processing).\n",
      "        *   **Wikipedia:** A large and diverse text corpus.\n",
      "        *   **Books3:** A large collection of books.\n",
      "        *   **IMDB Reviews:** For sentiment analysis.\n",
      "        *   **Reuters News:** For text classification.\n",
      "    *   **Create Your Own Dataset:** If you need data specific to your task, you may need to create your own dataset by scraping websites, collecting user-generated content, or manually labeling data.\n",
      "*   **Data Cleaning and Preprocessing:** This is a critical step.\n",
      "    *   **Remove irrelevant content:**  HTML tags, ads, boilerplate text.\n",
      "    *   **Handle special characters and encoding issues.**\n",
      "    *   **Tokenization:**  Break the text into tokens (words, subwords, characters).  Use a tokenizer library like:\n",
      "        *   **Hugging Face Tokenizers:**  Fast and versatile.\n",
      "        *   **spaCy:**  Good for general NLP tasks.\n",
      "        *   **NLTK:**  A classic NLP library.\n",
      "    *   **Lowercasing (optional):**  Convert all text to lowercase (can improve performance in some cases).\n",
      "    *   **Stemming/Lemmatization (optional):**  Reduce words to their root form (can be helpful for some tasks).\n",
      "    *   **Create Vocabulary:**  Build a vocabulary of all the unique tokens in your dataset.\n",
      "    *   **Numericalization:**  Convert the tokens into numerical IDs that the model can understand.\n",
      "*   **Split Data:** Divide your data into training, validation, and test sets.  A common split is 70% training, 15% validation, and 15% test.\n",
      "\n",
      "**3. Choose a Model Architecture:**\n",
      "\n",
      "*   **Simplified Architectures (Good for Beginners):**\n",
      "    *   **Recurrent Neural Networks (RNNs):**  Like LSTMs or GRUs.  Easier to understand than Transformers, but less powerful for long sequences.  Good for simple text generation or classification tasks.\n",
      "    *   **N-gram Models:**  Simple statistical models that predict the next word based on the previous N words.  Easy to implement but limited in their ability to capture long-range dependencies.\n",
      "*   **Transformer-Based Architectures (More Advanced):**\n",
      "    *   **Transformer Encoder:**  For tasks like text classification, sentiment analysis, and question answering.\n",
      "    *   **Transformer Decoder (GPT-style):**  For text generation.\n",
      "    *   **Hugging Face Transformers Library:**  Provides pre-trained Transformer models and tools for fine-tuning and training your own models.  This is the recommended approach for most projects.\n",
      "*   **Consider Pre-trained Models:**  Leverage pre-trained models from Hugging Face Model Hub.  Fine-tuning a pre-trained model is often much faster and requires less data than training a model from scratch.  Examples:\n",
      "    *   **BERT:**  A powerful encoder-based model for various NLP tasks.\n",
      "    *   **GPT-2/GPT-3:**  Decoder-based models for text generation.\n",
      "    *   **DistilBERT:**  A smaller, faster version of BERT.\n",
      "\n",
      "**4. Implement and Train Your Model:**\n",
      "\n",
      "*   **Choose a Deep Learning Framework:**\n",
      "    *   **PyTorch:**  More flexible and research-oriented.\n",
      "    *   **TensorFlow:**  More production-oriented.\n",
      "*   **Write Training Code:**\n",
      "    *   **Define the model architecture.**\n",
      "    *   **Define the loss function (e.g., cross-entropy loss).**\n",
      "    *   **Choose an optimizer (e.g., Adam).**\n",
      "    *   **Implement the training loop:**\n",
      "        *   Load a batch of data.\n",
      "        *   Pass the data through the model.\n",
      "        *   Calculate the loss.\n",
      "        *   Compute gradients.\n",
      "        *   Update the model's parameters.\n",
      "    *   **Implement validation:**  Evaluate the model on the validation set after each epoch to monitor its performance and prevent overfitting.\n",
      "*   **Use a GPU (if possible):**  Training deep learning models is much faster on a GPU.  Consider using cloud-based GPU services like Google Colab, AWS SageMaker, or Azure Machine Learning.\n",
      "*   **Monitor Training:**  Track the loss, accuracy, and other metrics during training to ensure that the model is learning properly.  Use tools like TensorBoard or Weights & Biases to visualize the training process.\n",
      "*   **Experiment with Hyperparameters:**  Adjust the learning rate, batch size, number of epochs, and other hyperparameters to optimize the model's performance.\n",
      "\n",
      "**5. Evaluate and Refine Your Model:**\n",
      "\n",
      "*   **Evaluate on the Test Set:**  After training, evaluate the model on the test set to get an unbiased estimate of its performance.\n",
      "*   **Analyze Results:**  Look at the model's predictions and identify areas where it is performing well and areas where it is struggling.\n",
      "*   **Fine-tune:**  If the model's performance is not satisfactory, try fine-tuning it on a specific task or dataset.\n",
      "*   **Data Augmentation:**  Increase the size and diversity of your training data by using data augmentation techniques.\n",
      "*   **Regularization:**  Use regularization techniques like dropout or weight decay to prevent overfitting.\n",
      "*   **Architecture Changes:**  Experiment with different model architectures or hyperparameters to improve performance.\n",
      "\n",
      "**Example: Building a Simple Text Generation Model with PyTorch and RNNs**\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "\n",
      "# 1. Data Preparation (Simplified)\n",
      "text = \"This is a simple example text for training a language model.\"\n",
      "tokens = list(set(text))  # Unique characters\n",
      "char_to_index = {char: i for i, char in enumerate(tokens)}\n",
      "index_to_char = {i: char for i, char in enumerate(tokens)}\n",
      "vocab_size = len(tokens)\n",
      "\n",
      "# Convert text to numerical data\n",
      "data = [char_to_index[char] for char in text]\n",
      "\n",
      "# 2. Model Definition\n",
      "class RNN(nn.Module):\n",
      "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
      "        super(RNN, self).__init__()\n",
      "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
      "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
      "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
      "\n",
      "    def forward(self, x, hidden):\n",
      "        embedded = self.embedding(x)\n",
      "        out, hidden = self.rnn(embedded, hidden)\n",
      "        out = self.linear(out)\n",
      "        return out, hidden\n",
      "\n",
      "# 3. Training\n",
      "embedding_dim = 8\n",
      "hidden_dim = 16\n",
      "model = RNN(vocab_size, embedding_dim, hidden_dim)\n",
      "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
      "loss_fn = nn.CrossEntropyLoss()\n",
      "\n",
      "# Prepare data for training (example: sequences of length 5)\n",
      "seq_length = 5\n",
      "input_sequences = [data[i:i+seq_length] for i in range(len(data) - seq_length)]\n",
      "target_sequences = [data[i+1:i+seq_length+1] for i in range(len(data) - seq_length)]\n",
      "\n",
      "num_epochs = 100\n",
      "for epoch in range(num_epochs):\n",
      "    total_loss = 0\n",
      "    for i in range(len(input_sequences)):\n",
      "        input_seq = torch.tensor([input_sequences[i]]).long()\n",
      "        target_seq = torch.tensor([target_sequences[i]]).long()\n",
      "\n",
      "        hidden = torch.zeros(1, 1, hidden_dim)  # Initialize hidden state\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "        output, hidden = model(input_seq, hidden)\n",
      "        loss = loss_fn(output.view(-1, vocab_size), target_seq.view(-1))\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        total_loss += loss.item()\n",
      "\n",
      "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(input_sequences)}\")\n",
      "\n",
      "# 4. Generation (Simplified)\n",
      "def generate_text(model, start_char, length=100):\n",
      "    model.eval()  # Set to evaluation mode\n",
      "    generated_text = start_char\n",
      "    input_index = torch.tensor([[char_to_index[start_char]]]).long()\n",
      "    hidden = torch.zeros(1, 1, hidden_dim)\n",
      "\n",
      "    with torch.no_grad():\n",
      "        for _ in range(length):\n",
      "            output, hidden = model(input_index, hidden)\n",
      "            predicted_index = torch.argmax(output[:, -1, :]).item()\n",
      "            predicted_char = index_to_char[predicted_index]\n",
      "            generated_text += predicted_char\n",
      "            input_index = torch.tensor([[predicted_index]]).long()\n",
      "\n",
      "    return generated_text\n",
      "\n",
      "# Example generation\n",
      "print(generate_text(model, \"T\", length=50))\n",
      "```\n",
      "\n",
      "**Key Considerations:**\n",
      "\n",
      "*   **Start Small:** Don't try to build a massive model right away. Begin with a simple architecture and a small dataset.\n",
      "*   **Learn the Fundamentals:** Understand the underlying concepts of neural networks, NLP, and deep learning.\n",
      "*   **Use Existing Libraries:** Leverage libraries like Hugging Face Transformers, PyTorch, and TensorFlow to simplify the development process.\n",
      "*   **Experiment and Iterate:**  Machine learning is an iterative process. Experiment with different architectures, hyperparameters, and training techniques to find what works best for your task.\n",
      "*   **Be Patient:** Training language models can take a significant amount of time and resources.\n",
      "\n",
      "Building a language model is a challenging but rewarding project. By following these steps and continuously learning, you can gain a deeper understanding of NLP and create your own intelligent systems. Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(output['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023607e",
   "metadata": {},
   "source": [
    "### Memory for Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84e96d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_graph = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "241b149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10f12ca30>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_graph.add_node('node1', model_call)\n",
    "memory_graph.set_entry_point('node1')\n",
    "memory_graph.set_finish_point('node1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ecc84c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_graph = memory_graph.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca4cd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'configurable':{'thread_id':1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06e814a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = memory_graph.invoke({'messages': {'role':'user','content':'which is the most clean contry'}}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e80f322d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='can you explain me how you get build', additional_kwargs={}, response_metadata={}, id='90e22518-79c9-4a65-b87e-4aa52f72b49d')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3f5a6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "who are you\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am a large language model, trained by Google.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "which is the most clean contry\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's a really interesting question! It's tough to say definitively which country is \"the most clean\" because cleanliness can be measured in different ways. Here's a breakdown of factors and some countries often considered to be very clean:\n",
      "\n",
      "**Factors to Consider:**\n",
      "\n",
      "*   **Environmental Performance Index (EPI):** This index, produced by Yale and Columbia Universities, looks at environmental health and ecosystem vitality. It considers factors like air and water quality, sanitation, and pollution levels.\n",
      "*   **Waste Management:** How well a country collects, processes, and disposes of waste is a major factor. Recycling rates, landfill management, and efforts to reduce waste generation are all important.\n",
      "*   **Air and Water Quality:** Clean air and water are essential for a healthy environment and population.\n",
      "*   **Public Sanitation:** Access to clean toilets, sewage systems, and hygiene education are crucial.\n",
      "*   **Public Perception:** How clean a country *feels* to residents and visitors can also play a role. This is influenced by things like street cleaning, litter control, and the overall appearance of public spaces.\n",
      "\n",
      "**Countries Often Ranked Highly for Cleanliness:**\n",
      "\n",
      "*   **Denmark:** Consistently scores high on the EPI, with strong performance in waste management and air quality.\n",
      "*   **Switzerland:** Known for its pristine landscapes, efficient waste management, and high environmental standards.\n",
      "*   **Luxembourg:** A small country with a strong focus on environmental sustainability and clean energy.\n",
      "*   **United Kingdom:** Has made significant strides in improving air and water quality and reducing pollution.\n",
      "*   **France:** Strong environmental policies and a commitment to sustainable development.\n",
      "*   **Austria:** High recycling rates and a focus on renewable energy.\n",
      "*   **Finland:** Known for its clean air and water, and its commitment to preserving its natural environment.\n",
      "*   **Sweden:** A leader in sustainable practices and waste management.\n",
      "*   **Norway:** Focuses on renewable energy and environmental protection.\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Data Availability:** It's not always easy to get accurate and comparable data for all countries.\n",
      "*   **Regional Variations:** Cleanliness can vary significantly within a country.\n",
      "*   **Subjectivity:** What one person considers clean, another might not.\n",
      "\n",
      "**In conclusion, while there's no single \"most clean\" country, Denmark, Switzerland, and Luxembourg are consistently ranked highly due to their strong environmental performance, efficient waste management, and commitment to sustainability.** It's best to look at the various indices and factors to get a more complete picture.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "which is the most clean contry\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As I mentioned before, determining the \"most clean\" country is complex because cleanliness is measured in various ways. However, based on factors like environmental performance, waste management, air and water quality, and public sanitation, **Denmark** is often considered one of the cleanest countries in the world. It consistently scores high on the Environmental Performance Index (EPI) and has strong performance in waste management and air quality.\n",
      "\n",
      "Other countries that are frequently ranked among the cleanest include Switzerland, Luxembourg, and Finland.\n"
     ]
    }
   ],
   "source": [
    "for i in state['messages']:\n",
    "    i.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage('w')]\n",
    "memory_graph.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbd6cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in memory.list(config):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3038cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.get(con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f5c40ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_prompts(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "daf4bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<async_generator object Pregel.aget_state_history at 0x10f113d30>\n"
     ]
    }
   ],
   "source": [
    "print(memory_graph.aget_state_history(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae714a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coroutine object Pregel.aget_state at 0x10f5f2140>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/81/8b_lw5jj52qfv4kmf14tk0p00000gn/T/ipykernel_36552/3467784115.py:1: RuntimeWarning: coroutine 'Pregel.aget_state' was never awaited\n",
      "  print(memory_graph.aget_state(config=config))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "print(memory_graph.aget_state(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abf56773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='who are you', additional_kwargs={}, response_metadata={}, id='460aa9c5-96d5-4ab2-98ae-ff2b31c68326'), AIMessage(content='I am a large language model, trained by Google.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--aeb98977-bd8e-4f1d-ae4e-b50470fc9755-0', usage_metadata={'input_tokens': 3, 'output_tokens': 12, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='which is the most clean contry', additional_kwargs={}, response_metadata={}, id='6196b180-8f33-4f4d-8d8d-198024f135e4'), AIMessage(content='That\\'s a really interesting question! It\\'s tough to say definitively which country is \"the most clean\" because cleanliness can be measured in different ways. Here\\'s a breakdown of factors and some countries often considered to be very clean:\\n\\n**Factors to Consider:**\\n\\n*   **Environmental Performance Index (EPI):** This index, produced by Yale and Columbia Universities, looks at environmental health and ecosystem vitality. It considers factors like air and water quality, sanitation, and pollution levels.\\n*   **Waste Management:** How well a country collects, processes, and disposes of waste is a major factor. Recycling rates, landfill management, and efforts to reduce waste generation are all important.\\n*   **Air and Water Quality:** Clean air and water are essential for a healthy environment and population.\\n*   **Public Sanitation:** Access to clean toilets, sewage systems, and hygiene education are crucial.\\n*   **Public Perception:** How clean a country *feels* to residents and visitors can also play a role. This is influenced by things like street cleaning, litter control, and the overall appearance of public spaces.\\n\\n**Countries Often Ranked Highly for Cleanliness:**\\n\\n*   **Denmark:** Consistently scores high on the EPI, with strong performance in waste management and air quality.\\n*   **Switzerland:** Known for its pristine landscapes, efficient waste management, and high environmental standards.\\n*   **Luxembourg:** A small country with a strong focus on environmental sustainability and clean energy.\\n*   **United Kingdom:** Has made significant strides in improving air and water quality and reducing pollution.\\n*   **France:** Strong environmental policies and a commitment to sustainable development.\\n*   **Austria:** High recycling rates and a focus on renewable energy.\\n*   **Finland:** Known for its clean air and water, and its commitment to preserving its natural environment.\\n*   **Sweden:** A leader in sustainable practices and waste management.\\n*   **Norway:** Focuses on renewable energy and environmental protection.\\n\\n**Important Considerations:**\\n\\n*   **Data Availability:** It\\'s not always easy to get accurate and comparable data for all countries.\\n*   **Regional Variations:** Cleanliness can vary significantly within a country.\\n*   **Subjectivity:** What one person considers clean, another might not.\\n\\n**In conclusion, while there\\'s no single \"most clean\" country, Denmark, Switzerland, and Luxembourg are consistently ranked highly due to their strong environmental performance, efficient waste management, and commitment to sustainability.** It\\'s best to look at the various indices and factors to get a more complete picture.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--b29eb21b-7a0c-4d00-9837-bb8dfda961a4-0', usage_metadata={'input_tokens': 21, 'output_tokens': 534, 'total_tokens': 555, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='which is the most clean contry', additional_kwargs={}, response_metadata={}, id='06661e50-272a-4a27-812b-44d913ccd498'), AIMessage(content='As I mentioned before, determining the \"most clean\" country is complex because cleanliness is measured in various ways. However, based on factors like environmental performance, waste management, air and water quality, and public sanitation, **Denmark** is often considered one of the cleanest countries in the world. It consistently scores high on the Environmental Performance Index (EPI) and has strong performance in waste management and air quality.\\n\\nOther countries that are frequently ranked among the cleanest include Switzerland, Luxembourg, and Finland.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--0c94f9c3-bfc7-468b-ba1b-94e93aba66c8-0', usage_metadata={'input_tokens': 561, 'output_tokens': 100, 'total_tokens': 661, 'input_token_details': {'cache_read': 0}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f02a1b8-ca05-6b0e-8007-a6a1d53472f9'}}, metadata={'source': 'loop', 'writes': {'node1': {'messages': [AIMessage(content='As I mentioned before, determining the \"most clean\" country is complex because cleanliness is measured in various ways. However, based on factors like environmental performance, waste management, air and water quality, and public sanitation, **Denmark** is often considered one of the cleanest countries in the world. It consistently scores high on the Environmental Performance Index (EPI) and has strong performance in waste management and air quality.\\n\\nOther countries that are frequently ranked among the cleanest include Switzerland, Luxembourg, and Finland.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--0c94f9c3-bfc7-468b-ba1b-94e93aba66c8-0', usage_metadata={'input_tokens': 561, 'output_tokens': 100, 'total_tokens': 661, 'input_token_details': {'cache_read': 0}})]}}, 'step': 7, 'parents': {}, 'thread_id': 1}, created_at='2025-05-06T01:43:46.028793+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f02a1b8-bea8-6f5e-8006-c91552ed0d8c'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52406d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

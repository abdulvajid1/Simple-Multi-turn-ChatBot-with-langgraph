{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87ce03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulvajid/AI/PROJECTS/Simple-Multi-turn-ChatBot-with-langgraph/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "from langgraph.graph.message import AnyMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph.message import add_messages, MessagesState\n",
    "from typing import Annotated\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343aabdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f89f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b710b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77495135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a2d9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_call(state):\n",
    "    return {\"messages\" : [llm.invoke(state['messages'])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947056a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1082cbca0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.set_entry_point('node1')\n",
    "graph.add_node('node1', model_call)\n",
    "graph.add_edge('node1', END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bde526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57fe7c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langgraph.graph.state.CompiledStateGraph"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "011ebb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage('can you explain me how you get build')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c409c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.invoke({'messages': messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f9820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = [HumanMessage('ok now explain me how can i do it')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53da8b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='can you explain me how you get build', additional_kwargs={}, response_metadata={}, id='28efb95c-a2f1-4bb9-9d12-ba979ee3a105'),\n",
       " AIMessage(content='Okay, let\\'s break down how I, as a large language model, am \"built.\" It\\'s a multi-faceted process involving data, architecture, training, and ongoing refinement.  Think of it like building a house: you need materials (data), a blueprint (architecture), construction workers (training algorithms), and ongoing maintenance (refinement).\\n\\n**1. Data: The Foundation**\\n\\n*   **Massive Text and Code Datasets:** I am trained on an enormous amount of text and code data. This data comes from a variety of sources, including:\\n    *   **The Public Web:**  Web pages, articles, blog posts, forums, and other publicly available text.\\n    *   **Books:**  A vast collection of books covering a wide range of subjects.\\n    *   **Code Repositories:**  Code from platforms like GitHub, covering various programming languages.\\n    *   **Other Licensed Datasets:**  Datasets specifically created or licensed for training language models.\\n*   **Data Cleaning and Preprocessing:** The raw data is often noisy and inconsistent.  It undergoes a cleaning and preprocessing phase to:\\n    *   **Remove irrelevant content:**  Ads, boilerplate text, and other unwanted elements are filtered out.\\n    *   **Normalize text:**  Convert text to a consistent format (e.g., lowercase, handling punctuation).\\n    *   **Tokenize text:**  Break down the text into smaller units called \"tokens.\"  Tokens can be words, parts of words (subwords), or even individual characters.  This is crucial for the model to understand and process the text.\\n*   **Data Augmentation (Sometimes):**  Techniques to artificially increase the size and diversity of the training data.  This can involve:\\n    *   **Back-translation:** Translating text to another language and then back to the original language to create slightly different versions.\\n    *   **Synonym replacement:** Replacing words with their synonyms.\\n    *   **Random insertion/deletion:**  Adding or removing words randomly.\\n\\n**2. Architecture: The Blueprint**\\n\\n*   **Transformer Networks:**  The core architecture used in most modern large language models, including me, is based on the \"Transformer\" neural network architecture.  Transformers are particularly well-suited for processing sequential data like text.\\n*   **Key Components of the Transformer:**\\n    *   **Attention Mechanism:**  The heart of the Transformer.  It allows the model to focus on the most relevant parts of the input sequence when processing each word.  Instead of treating all words equally, it assigns weights to different words based on their importance in the context.  This is what allows me to understand relationships between words that are far apart in a sentence.\\n    *   **Encoder:**  Processes the input sequence and creates a representation of it.\\n    *   **Decoder:**  Generates the output sequence (e.g., the response to your question).  Some models use only the decoder part of the Transformer (e.g., GPT models).\\n    *   **Feedforward Neural Networks:**  Apply non-linear transformations to the data.\\n    *   **Layer Normalization:**  Helps to stabilize training and improve performance.\\n*   **Model Size (Number of Parameters):**  The size of a language model is often measured by the number of parameters (weights and biases) in the neural network.  Larger models generally have more capacity to learn complex patterns in the data.  I have a very large number of parameters, allowing me to capture intricate relationships in language.\\n*   **Variations and Improvements:**  There are many variations and improvements to the basic Transformer architecture, such as:\\n    *   **Sparse Attention:**  Reduces the computational cost of the attention mechanism.\\n    *   **Mixture of Experts:**  Uses multiple sub-models (experts) to handle different types of data or tasks.\\n\\n**3. Training: The Construction Process**\\n\\n*   **Self-Supervised Learning:**  The primary training method is self-supervised learning.  This means the model learns from the data itself, without explicit labels.  A common approach is:\\n    *   **Masked Language Modeling (MLM):**  A portion of the input text is masked (replaced with a special token), and the model is trained to predict the masked words.  This forces the model to understand the context and relationships between words.\\n    *   **Next Sentence Prediction (NSP):**  The model is given two sentences and trained to predict whether the second sentence follows the first.  This helps the model understand relationships between sentences.\\n    *   **Causal Language Modeling:** The model is trained to predict the next word in a sequence, given the preceding words. This is used in generative models like GPT.\\n*   **Optimization Algorithms:**  Algorithms like Adam are used to adjust the model\\'s parameters during training to minimize the loss function (the difference between the model\\'s predictions and the actual values).\\n*   **Distributed Training:**  Training large language models requires massive computational resources.  Distributed training is used to split the training process across multiple GPUs or TPUs (Tensor Processing Units).\\n*   **Regularization Techniques:**  Techniques like dropout and weight decay are used to prevent overfitting (when the model learns the training data too well and performs poorly on new data).\\n*   **Training Data Iteration:** The model is exposed to the training data multiple times (epochs) to refine its understanding.\\n\\n**4. Refinement: Ongoing Maintenance and Improvement**\\n\\n*   **Fine-tuning:**  After the initial pre-training, the model can be fine-tuned on specific tasks or datasets.  This involves training the model on a smaller, more focused dataset to improve its performance on a particular task (e.g., question answering, text summarization, translation).\\n*   **Reinforcement Learning from Human Feedback (RLHF):**  A technique where human evaluators provide feedback on the model\\'s responses, and this feedback is used to train a reward model.  The reward model is then used to train the language model using reinforcement learning, encouraging it to generate responses that are more helpful, harmless, and honest.\\n*   **Continuous Learning:**  Language models are often continuously updated with new data and improved training techniques.  This allows them to stay up-to-date with current events and improve their overall performance.\\n*   **Safety and Bias Mitigation:**  Significant effort is put into identifying and mitigating potential biases in the model and ensuring that it generates safe and responsible responses.  This involves:\\n    *   **Bias detection and removal:**  Analyzing the model\\'s behavior to identify and remove biases.\\n    *   **Safety training:**  Training the model to avoid generating harmful or offensive content.\\n    *   **Red Teaming:**  Having experts try to \"break\" the model by prompting it to generate inappropriate responses.\\n\\n**In Summary:**\\n\\nI am built through a complex process involving:\\n\\n1.  **Gathering and preparing massive amounts of text and code data.**\\n2.  **Designing a powerful neural network architecture (Transformer).**\\n3.  **Training the model on the data using self-supervised learning techniques.**\\n4.  **Refining the model through fine-tuning, reinforcement learning, and continuous learning.**\\n5.  **Actively working to mitigate biases and ensure safety.**\\n\\nThe process is constantly evolving as researchers develop new techniques and architectures.  The goal is to create language models that are more intelligent, helpful, and beneficial to society.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--6a7289a0-2e4e-42f4-bb2b-72ca3be0bb5a-0', usage_metadata={'input_tokens': 8, 'output_tokens': 1565, 'total_tokens': 1573, 'input_token_details': {'cache_read': 0}}),\n",
       " HumanMessage(content='ok now explain me how can i do it', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state['messages'] + new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89487c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke({'messages': state['messages'] + new_message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eee5740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down how you can build your own language model. Keep in mind that building a state-of-the-art model like me requires significant resources (data, compute, expertise). However, you can definitely build smaller, simpler models to learn the fundamentals and experiment. Here's a roadmap:\n",
      "\n",
      "**1. Define Your Goal and Scope:**\n",
      "\n",
      "*   **What do you want your model to do?**  Be specific.  Examples:\n",
      "    *   **Text generation:** Generate creative text, stories, poems, code, etc.\n",
      "    *   **Text classification:** Classify text into categories (e.g., spam detection, sentiment analysis).\n",
      "    *   **Question answering:** Answer questions based on a given context.\n",
      "    *   **Translation:** Translate text from one language to another.\n",
      "    *   **Summarization:** Summarize long documents.\n",
      "*   **What is your target domain?**  Will it be general-purpose, or focused on a specific area (e.g., medical text, legal documents, financial news)?\n",
      "*   **What resources do you have available?**  Consider your computing power (CPU, GPU), data storage, and time commitment.\n",
      "*   **What is your level of expertise?**  Are you a beginner, intermediate, or advanced programmer with machine learning experience?\n",
      "\n",
      "**2. Data Acquisition and Preparation:**\n",
      "\n",
      "*   **Find or Create a Dataset:**  The quality and quantity of your data are crucial.\n",
      "    *   **Public Datasets:** Start with publicly available datasets. Some popular options:\n",
      "        *   **Hugging Face Datasets:** A vast collection of datasets for various NLP tasks.\n",
      "        *   **Common Crawl:** A massive archive of web pages (requires significant processing).\n",
      "        *   **Wikipedia:** A large and diverse text corpus.\n",
      "        *   **Books3:** A large collection of books.\n",
      "        *   **IMDB Reviews:** For sentiment analysis.\n",
      "        *   **Reuters News:** For text classification.\n",
      "    *   **Create Your Own Dataset:** If you need data specific to your task, you may need to create your own dataset by scraping websites, collecting user-generated content, or manually labeling data.\n",
      "*   **Data Cleaning and Preprocessing:** This is a critical step.\n",
      "    *   **Remove irrelevant content:**  HTML tags, ads, boilerplate text.\n",
      "    *   **Handle special characters and encoding issues.**\n",
      "    *   **Tokenization:**  Break the text into tokens (words, subwords, characters).  Use a tokenizer library like:\n",
      "        *   **Hugging Face Tokenizers:**  Fast and versatile.\n",
      "        *   **spaCy:**  Good for general NLP tasks.\n",
      "        *   **NLTK:**  A classic NLP library.\n",
      "    *   **Lowercasing (optional):**  Convert all text to lowercase (can improve performance in some cases).\n",
      "    *   **Stemming/Lemmatization (optional):**  Reduce words to their root form (can be helpful for some tasks).\n",
      "    *   **Create Vocabulary:**  Build a vocabulary of all the unique tokens in your dataset.\n",
      "    *   **Numericalization:**  Convert the tokens into numerical IDs that the model can understand.\n",
      "*   **Split Data:** Divide your data into training, validation, and test sets.  A common split is 70% training, 15% validation, and 15% test.\n",
      "\n",
      "**3. Choose a Model Architecture:**\n",
      "\n",
      "*   **Simplified Architectures (Good for Beginners):**\n",
      "    *   **Recurrent Neural Networks (RNNs):**  Like LSTMs or GRUs.  Easier to understand than Transformers, but less powerful for long sequences.  Good for simple text generation or classification tasks.\n",
      "    *   **N-gram Models:**  Simple statistical models that predict the next word based on the previous N words.  Easy to implement but limited in their ability to capture long-range dependencies.\n",
      "*   **Transformer-Based Architectures (More Advanced):**\n",
      "    *   **Transformer Encoder:**  For tasks like text classification, sentiment analysis, and question answering.\n",
      "    *   **Transformer Decoder (GPT-style):**  For text generation.\n",
      "    *   **Hugging Face Transformers Library:**  Provides pre-trained Transformer models and tools for fine-tuning and training your own models.  This is the recommended approach for most projects.\n",
      "*   **Consider Pre-trained Models:**  Leverage pre-trained models from Hugging Face Model Hub.  Fine-tuning a pre-trained model is often much faster and requires less data than training a model from scratch.  Examples:\n",
      "    *   **BERT:**  A powerful encoder-based model for various NLP tasks.\n",
      "    *   **GPT-2/GPT-3:**  Decoder-based models for text generation.\n",
      "    *   **DistilBERT:**  A smaller, faster version of BERT.\n",
      "\n",
      "**4. Implement and Train Your Model:**\n",
      "\n",
      "*   **Choose a Deep Learning Framework:**\n",
      "    *   **PyTorch:**  More flexible and research-oriented.\n",
      "    *   **TensorFlow:**  More production-oriented.\n",
      "*   **Write Training Code:**\n",
      "    *   **Define the model architecture.**\n",
      "    *   **Define the loss function (e.g., cross-entropy loss).**\n",
      "    *   **Choose an optimizer (e.g., Adam).**\n",
      "    *   **Implement the training loop:**\n",
      "        *   Load a batch of data.\n",
      "        *   Pass the data through the model.\n",
      "        *   Calculate the loss.\n",
      "        *   Compute gradients.\n",
      "        *   Update the model's parameters.\n",
      "    *   **Implement validation:**  Evaluate the model on the validation set after each epoch to monitor its performance and prevent overfitting.\n",
      "*   **Use a GPU (if possible):**  Training deep learning models is much faster on a GPU.  Consider using cloud-based GPU services like Google Colab, AWS SageMaker, or Azure Machine Learning.\n",
      "*   **Monitor Training:**  Track the loss, accuracy, and other metrics during training to ensure that the model is learning properly.  Use tools like TensorBoard or Weights & Biases to visualize the training process.\n",
      "*   **Experiment with Hyperparameters:**  Adjust the learning rate, batch size, number of epochs, and other hyperparameters to optimize the model's performance.\n",
      "\n",
      "**5. Evaluate and Refine Your Model:**\n",
      "\n",
      "*   **Evaluate on the Test Set:**  After training, evaluate the model on the test set to get an unbiased estimate of its performance.\n",
      "*   **Analyze Results:**  Look at the model's predictions and identify areas where it is performing well and areas where it is struggling.\n",
      "*   **Fine-tune:**  If the model's performance is not satisfactory, try fine-tuning it on a specific task or dataset.\n",
      "*   **Data Augmentation:**  Increase the size and diversity of your training data by using data augmentation techniques.\n",
      "*   **Regularization:**  Use regularization techniques like dropout or weight decay to prevent overfitting.\n",
      "*   **Architecture Changes:**  Experiment with different model architectures or hyperparameters to improve performance.\n",
      "\n",
      "**Example: Building a Simple Text Generation Model with PyTorch and RNNs**\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "\n",
      "# 1. Data Preparation (Simplified)\n",
      "text = \"This is a simple example text for training a language model.\"\n",
      "tokens = list(set(text))  # Unique characters\n",
      "char_to_index = {char: i for i, char in enumerate(tokens)}\n",
      "index_to_char = {i: char for i, char in enumerate(tokens)}\n",
      "vocab_size = len(tokens)\n",
      "\n",
      "# Convert text to numerical data\n",
      "data = [char_to_index[char] for char in text]\n",
      "\n",
      "# 2. Model Definition\n",
      "class RNN(nn.Module):\n",
      "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
      "        super(RNN, self).__init__()\n",
      "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
      "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
      "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
      "\n",
      "    def forward(self, x, hidden):\n",
      "        embedded = self.embedding(x)\n",
      "        out, hidden = self.rnn(embedded, hidden)\n",
      "        out = self.linear(out)\n",
      "        return out, hidden\n",
      "\n",
      "# 3. Training\n",
      "embedding_dim = 8\n",
      "hidden_dim = 16\n",
      "model = RNN(vocab_size, embedding_dim, hidden_dim)\n",
      "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
      "loss_fn = nn.CrossEntropyLoss()\n",
      "\n",
      "# Prepare data for training (example: sequences of length 5)\n",
      "seq_length = 5\n",
      "input_sequences = [data[i:i+seq_length] for i in range(len(data) - seq_length)]\n",
      "target_sequences = [data[i+1:i+seq_length+1] for i in range(len(data) - seq_length)]\n",
      "\n",
      "num_epochs = 100\n",
      "for epoch in range(num_epochs):\n",
      "    total_loss = 0\n",
      "    for i in range(len(input_sequences)):\n",
      "        input_seq = torch.tensor([input_sequences[i]]).long()\n",
      "        target_seq = torch.tensor([target_sequences[i]]).long()\n",
      "\n",
      "        hidden = torch.zeros(1, 1, hidden_dim)  # Initialize hidden state\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "        output, hidden = model(input_seq, hidden)\n",
      "        loss = loss_fn(output.view(-1, vocab_size), target_seq.view(-1))\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        total_loss += loss.item()\n",
      "\n",
      "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(input_sequences)}\")\n",
      "\n",
      "# 4. Generation (Simplified)\n",
      "def generate_text(model, start_char, length=100):\n",
      "    model.eval()  # Set to evaluation mode\n",
      "    generated_text = start_char\n",
      "    input_index = torch.tensor([[char_to_index[start_char]]]).long()\n",
      "    hidden = torch.zeros(1, 1, hidden_dim)\n",
      "\n",
      "    with torch.no_grad():\n",
      "        for _ in range(length):\n",
      "            output, hidden = model(input_index, hidden)\n",
      "            predicted_index = torch.argmax(output[:, -1, :]).item()\n",
      "            predicted_char = index_to_char[predicted_index]\n",
      "            generated_text += predicted_char\n",
      "            input_index = torch.tensor([[predicted_index]]).long()\n",
      "\n",
      "    return generated_text\n",
      "\n",
      "# Example generation\n",
      "print(generate_text(model, \"T\", length=50))\n",
      "```\n",
      "\n",
      "**Key Considerations:**\n",
      "\n",
      "*   **Start Small:** Don't try to build a massive model right away. Begin with a simple architecture and a small dataset.\n",
      "*   **Learn the Fundamentals:** Understand the underlying concepts of neural networks, NLP, and deep learning.\n",
      "*   **Use Existing Libraries:** Leverage libraries like Hugging Face Transformers, PyTorch, and TensorFlow to simplify the development process.\n",
      "*   **Experiment and Iterate:**  Machine learning is an iterative process. Experiment with different architectures, hyperparameters, and training techniques to find what works best for your task.\n",
      "*   **Be Patient:** Training language models can take a significant amount of time and resources.\n",
      "\n",
      "Building a language model is a challenging but rewarding project. By following these steps and continuously learning, you can gain a deeper understanding of NLP and create your own intelligent systems. Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(output['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023607e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
